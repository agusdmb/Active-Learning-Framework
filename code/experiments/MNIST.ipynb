{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Lerning sobre el MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Del Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import ActiveLearner, Dataset, Oracle\n",
    "from querys import UncertaintySelector, CertaintySelector, RandomSelector, MinDiffSelector, EntropySelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "Para probar el framework lo haré sobre el MNIST. Aprovechando que está todo etiquetado, dejaré un 25% del total para hacer el testing al final de cada iteración. De los datos restantes, tomaré el 5% como datos etiquetados iniciales, y usaré el otro 95% para aplicar el aprendizaje activo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.33, random_state=3)\n",
    "X_train, X_unlabeled, y_train, y_unlabeled = train_test_split(X, y, test_size=0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instancias mínimas para poder aplicar el framework\n",
    "\n",
    "Creo una instancia para el **Dataset** y otra para el **Oracle**.\n",
    "\n",
    "El Oracle lo midifiqué para poder mostrar una imagen del dígito a etiquetar. Luego, para acelerar el proceso de testeo lo volví a modificar para que en lugar de preguntar al usuario etiquetador, directamente devuelva la etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsOracle(Oracle):\n",
    "    target_names = list(range(10))\n",
    "    \n",
    "    def ask(self, X_readable, recoms):\n",
    "        return X_readable\n",
    "\n",
    "    def show_element(self, x, r):\n",
    "        plt.gray()\n",
    "        plt.matshow(x[0])\n",
    "        plt.show()\n",
    "        print(r)\n",
    "        print(x[1])\n",
    "\n",
    "class Digits(Dataset):\n",
    "    real_y = y_unlabeled\n",
    "    \n",
    "    def get_unlabeled_readable(self, i):\n",
    "        return self.real_y[i]\n",
    "        #x = self.get_unlabeled()[i]\n",
    "        #return (x.reshape(8,8), self.real_y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparación de la cantidad de datos *iniciales*, para *validación* dentro de cada iteración y para para *testing* final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(X_unlabeled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación Final\n",
    "\n",
    "Comparación de los selectores de Active Learning y también del selector RandomSelector (el cual es equivalente a no aplicar Active Learning. Se compara una vez a cada uno de los selectores del framework excepto el RandomSelector, que es el unico que tiene un comportamiento no determinístico. Por esta razón se ejecutan 5 instancias del mismo y luego se calcula un promedio de ellos para ver el desepempeño medio del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectors = [CertaintySelector, UncertaintySelector, MinDiffSelector, EntropySelector, RandomSelector, RandomSelector, RandomSelector, RandomSelector, RandomSelector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selector_scores = []\n",
    "for selector in selectors:\n",
    "    dataset = Digits(X_train, y_train, X_unlabeled)\n",
    "    forest = RandomForestClassifier(random_state=7)\n",
    "    #forest = LogisticRegression()\n",
    "    oracle = DigitsOracle()\n",
    "    al = ActiveLearner(forest, dataset, selector, oracle)\n",
    "    scores = []\n",
    "    al.fit()\n",
    "    scores.append(al.model.score(X_test, y_test))\n",
    "    for _ in tqdm(range(120)):\n",
    "        selected = al.select(10)\n",
    "        y = al.ask(selected)\n",
    "        al.tag_elements(selected, y)\n",
    "        al.fit()\n",
    "        scores.append(al.model.score(X_test, y_test))\n",
    "    selector_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_avg = []\n",
    "for i in range(len(selector_scores[0])):\n",
    "    suma = 0\n",
    "    for selector in selector_scores[4:]:\n",
    "        suma += selector[i]\n",
    "    random_avg.append(suma/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaciones\n",
    "\n",
    "Grafico el desempeño de cada uno de los selectores y el promedio de los RandomSelectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "plt.title('Comparación de selectores')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Precision')\n",
    "plt.plot(selector_scores[0])\n",
    "plt.plot(selector_scores[1])\n",
    "plt.plot(selector_scores[2])\n",
    "plt.plot(selector_scores[3])\n",
    "plt.plot(selector_scores[4], color='gray')\n",
    "plt.plot(selector_scores[5], color='gray')\n",
    "plt.plot(selector_scores[6], color='gray')\n",
    "plt.plot(selector_scores[7], color='gray')\n",
    "plt.plot(selector_scores[8], color='gray')\n",
    "#plt.plot(random_avg, color='black')\n",
    "plt.legend(['Certainty', 'Uncertainty', 'MinDiff', 'Entropy', 'Random'])\n",
    "plt.savefig('mnist_random_forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "tesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
